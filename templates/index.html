<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Analysis for AI Image Generation</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {
            padding-top: 2rem;
            background-color: #f8f9fa;
        }

        .recording {
            animation: pulse 1.5s infinite;
            background-color: #dc3545 !important;
            border-color: #dc3545 !important;
        }

        @keyframes pulse {
            0% {
                opacity: 1;
            }

            50% {
                opacity: 0.7;
            }

            100% {
                opacity: 1;
            }
        }

        .result-card {
            margin-bottom: 1rem;
            transition: all 0.3s ease;
        }

        .result-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.1);
        }

        .confidence-badge {
            position: absolute;
            top: 10px;
            right: 10px;
        }

        #audioVisualization {
            width: 100%;
            height: 60px;
            background-color: #f1f3f5;
            border-radius: 4px;
            margin-bottom: 1rem;
            position: relative;
            overflow: hidden;
        }

        .visualizer-bar {
            position: absolute;
            bottom: 0;
            background-color: #6c757d;
            width: 3px;
            margin-right: 1px;
            border-radius: 1px 1px 0 0;
        }

        #loadingIndicator {
            display: none;
        }

        .copy-btn {
            position: absolute;
            bottom: 10px;
            right: 10px;
            opacity: 0.7;
        }

        .copy-btn:hover {
            opacity: 1;
        }

        #audioPlayer {
            width: 100%;
            margin-top: 1rem;
        }

        #recordingControls {
            display: flex;
            gap: 10px;
            margin-bottom: 15px;
        }

        #recordingControls button {
            flex: 1;
        }

        #transcriptContainer {
            margin-top: 15px;
            padding: 10px;
            background-color: #f1f3f5;
            border-radius: 4px;
            min-height: 60px;
        }

        .speech-status {
            font-style: italic;
            color: #6c757d;
        }

        #transcriptText {
            font-weight: 500;
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-8">
                <div class="card shadow">
                    <div class="card-header bg-primary text-white">
                        <h3 class="mb-0">Audio Analysis for AI Image Generation</h3>
                    </div>
                    <div class="card-body">
                        <p class="card-text">Speak or upload an audio file to analyze and generate AI image prompts</p>

                        <ul class="nav nav-tabs mb-3" id="inputTabs" role="tablist">
                            <li class="nav-item" role="presentation">
                                <button class="nav-link active" id="record-tab" data-bs-toggle="tab"
                                    data-bs-target="#record" type="button" role="tab">Record Audio</button>
                            </li>
                            <li class="nav-item" role="presentation">
                                <button class="nav-link" id="upload-tab" data-bs-toggle="tab" data-bs-target="#upload"
                                    type="button" role="tab">Upload Audio</button>
                            </li>
                        </ul>

                        <div class="tab-content" id="inputTabsContent">
                            <!-- Record Audio Tab -->
                            <div class="tab-pane fade show active" id="record" role="tabpanel">
                                <div id="audioVisualization"></div>
                                <div id="recordingControls">
                                    <button id="recordButton" class="btn btn-primary btn-lg">
                                        <i class="bi bi-mic-fill"></i> Record
                                    </button>
                                    <button id="pauseButton" class="btn btn-warning btn-lg" disabled>
                                        Pause
                                    </button>
                                    <button id="stopButton" class="btn btn-danger btn-lg" disabled>
                                        Stop
                                    </button>
                                </div>
                                <div id="formats" class="small text-muted mb-3">Format: Start recording to see sample rate</div>
                                <audio id="audioPlayer" controls></audio>
                                
                                <!-- Live Transcript Section -->
                                <div id="transcriptContainer" class="mt-3">
                                    <h5 class="mb-2">Live Transcript</h5>
                                    <p id="speechStatus" class="speech-status">Ready to record...</p>
                                    <div id="transcriptText"></div>
                                </div>
                                
                                <p class="text-muted mt-2">
                                    <small>Speak clearly for better results. Press stop when finished to analyze.</small>
                                </p>
                            </div>

                            <!-- Upload Audio Tab -->
                            <div class="tab-pane fade" id="upload" role="tabpanel">
                                <form id="uploadForm" enctype="multipart/form-data">
                                    <div class="mb-3">
                                        <label for="audioFile" class="form-label">Select Audio File</label>
                                        <input class="form-control" type="file" id="audioFile" accept=".wav,.mp3,.ogg,.m4a">
                                        <div class="form-text">Supported formats: WAV, MP3, OGG, M4A</div>
                                    </div>
                                    <div class="d-grid">
                                        <button type="submit" class="btn btn-primary btn-lg">Analyze Audio</button>
                                    </div>
                                </form>
                            </div>
                        </div>

                        <!-- Loading Indicator -->
                        <div id="loadingIndicator" class="text-center my-4">
                            <div class="spinner-border text-primary" role="status">
                                <span class="visually-hidden">Loading...</span>
                            </div>
                            <p class="mt-2">Analyzing audio...</p>
                        </div>
                    </div>
                </div>

                <!-- Results Section -->
                <div id="resultsContainer" class="mt-4" style="display: none;">
                    <h4>Analysis Results</h4>
                    <div id="resultsContent"></div>
                </div>
            </div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.rawgit.com/mattdiamond/Recorderjs/08e7abd9/dist/recorder.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            // DOM elements
            const recordButton = document.getElementById('recordButton');
            const pauseButton = document.getElementById('pauseButton');
            const stopButton = document.getElementById('stopButton');
            const uploadForm = document.getElementById('uploadForm');
            const audioFileInput = document.getElementById('audioFile');
            const loadingIndicator = document.getElementById('loadingIndicator');
            const resultsContainer = document.getElementById('resultsContainer');
            const resultsContent = document.getElementById('resultsContent');
            const audioVisualization = document.getElementById('audioVisualization');
            const audioPlayer = document.getElementById('audioPlayer');
            const formatsDisplay = document.getElementById('formats');
            const transcriptText = document.getElementById('transcriptText');
            const speechStatus = document.getElementById('speechStatus');

            // Variables for recording
            let gumStream;           // Stream from getUserMedia()
            let rec;                 // Recorder.js object
            let input;               // MediaStreamAudioSourceNode
            let audioContext;        // AudioContext
            let analyser;            // AnalyserNode for visualization
            let visualizerBars = []; // Array to hold visualization bars
            let animationFrame;      // For visualization animation
            let recordedBlob;        // To store the recorded audio blob
            let finalTranscript = ''; // To store the final transcript
            let recognition;         // SpeechRecognition object
            let isRecognitionActive = false;

            // Check browser support for SpeechRecognition
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            const SpeechGrammarList = window.SpeechGrammarList || window.webkitSpeechGrammarList;
            const SpeechRecognitionEvent = window.SpeechRecognitionEvent || window.webkitSpeechRecognitionEvent;

            const hasSpeechRecognition = !!SpeechRecognition;
            
            if (!hasSpeechRecognition) {
                speechStatus.textContent = "Speech recognition not supported in this browser.";
                speechStatus.style.color = "#dc3545";
            }

            // Initialize speech recognition
            function initSpeechRecognition() {
                if (!hasSpeechRecognition) return;
                
                recognition = new SpeechRecognition();
                recognition.continuous = true;
                recognition.interimResults = true;
                recognition.lang = 'th-TH'; // Set language - can be made configurable
                
                recognition.onstart = function() {
                    speechStatus.textContent = "Listening...";
                    isRecognitionActive = true;
                };
                
                recognition.onresult = function(event) {
                    let interim = '';
                    
                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        if (event.results[i].isFinal) {
                            finalTranscript += event.results[i][0].transcript + ' ';
                        } else {
                            interim += event.results[i][0].transcript;
                        }
                    }
                    
                    transcriptText.innerHTML = 
                        finalTranscript + 
                        '<span style="color: #6c757d;">' + interim + '</span>';
                };
                
                recognition.onerror = function(event) {
                    console.error('Speech recognition error', event.error);
                    speechStatus.textContent = "Error: " + event.error;
                    speechStatus.style.color = "#dc3545";
                };
                
                recognition.onend = function() {
                    if (isRecognitionActive) {
                        // If we're still supposed to be recording, restart recognition
                        // This handles the automatic restart when recognition times out
                        recognition.start();
                    } else {
                        speechStatus.textContent = "Stopped listening.";
                    }
                };
            }

            // Initialize visualization
            function initAudioVisualization() {
                // Create bars for visualization
                audioVisualization.innerHTML = '';
                const barCount = 40;
                for (let i = 0; i < barCount; i++) {
                    const bar = document.createElement('div');
                    bar.className = 'visualizer-bar';
                    bar.style.left = (i * 4) + 'px';
                    bar.style.height = '0px';
                    audioVisualization.appendChild(bar);
                    visualizerBars.push(bar);
                }
            }

            // Visualize audio function
            function visualizeAudio() {
                if (!analyser) return;

                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);
                analyser.getByteFrequencyData(dataArray);

                const step = Math.floor(bufferLength / visualizerBars.length);

                for (let i = 0; i < visualizerBars.length; i++) {
                    const value = dataArray[i * step];
                    const height = (value / 255) * audioVisualization.clientHeight;
                    visualizerBars[i].style.height = height + 'px';
                }

                animationFrame = requestAnimationFrame(visualizeAudio);
            }

            // Start recording function
            function startRecording() {
                // Reset UI and transcript
                recordButton.classList.add('recording');
                recordButton.disabled = true;
                pauseButton.disabled = false;
                stopButton.disabled = false;
                finalTranscript = '';
                transcriptText.textContent = '';
                speechStatus.textContent = "Initializing...";
                speechStatus.style.color = "#6c757d";
                
                // Get audio stream
                const constraints = { audio: true, video: false };
                
                navigator.mediaDevices.getUserMedia(constraints)
                    .then(function(stream) {
                        // Create audio context
                        audioContext = new (window.AudioContext || window.webkitAudioContext)();
                        
                        // Update format display
                        formatsDisplay.innerHTML = "Format: 1 channel pcm @ " + audioContext.sampleRate / 1000 + "kHz";
                        
                        // Create analyser for visualization
                        analyser = audioContext.createAnalyser();
                        analyser.fftSize = 256;
                        
                        // Store stream
                        gumStream = stream;
                        
                        // Create input source and connect to analyser
                        input = audioContext.createMediaStreamSource(stream);
                        input.connect(analyser);
                        
                        // Create recorder
                        rec = new Recorder(input, { numChannels: 1 });
                        
                        // Start recording
                        rec.record();
                        
                        // Start visualization
                        visualizeAudio();
                        
                        // Start speech recognition if available
                        if (hasSpeechRecognition) {
                            isRecognitionActive = true;
                            initSpeechRecognition();
                            recognition.start();
                        }
                    })
                    .catch(function(err) {
                        console.error("Error accessing microphone:", err);
                        alert("Could not access microphone. Please check permissions.");
                        recordButton.disabled = false;
                        pauseButton.disabled = true;
                        stopButton.disabled = true;
                        speechStatus.textContent = "Error accessing microphone.";
                        speechStatus.style.color = "#dc3545";
                    });
            }

            // Pause recording function
            function pauseRecording() {
                if (rec.recording) {
                    // Pause
                    rec.stop();
                    pauseButton.innerHTML = "Resume";
                    
                    // Pause speech recognition
                    if (hasSpeechRecognition && isRecognitionActive) {
                        recognition.stop();
                        isRecognitionActive = false;
                        speechStatus.textContent = "Paused.";
                    }
                } else {
                    // Resume
                    rec.record();
                    pauseButton.innerHTML = "Pause";
                    
                    // Resume speech recognition
                    if (hasSpeechRecognition) {
                        isRecognitionActive = true;
                        recognition.start();
                    }
                }
            }

            // Stop recording function
            function stopRecording() {
                // Update UI
                stopButton.disabled = true;
                recordButton.disabled = false;
                pauseButton.disabled = true;
                pauseButton.innerHTML = "Pause";
                recordButton.classList.remove('recording');
                
                // Stop recording
                rec.stop();
                
                // Stop stream tracks
                gumStream.getAudioTracks()[0].stop();
                
                // Stop visualization
                cancelAnimationFrame(animationFrame);
                visualizerBars.forEach(bar => bar.style.height = '0px');
                
                // Stop speech recognition
                if (hasSpeechRecognition && isRecognitionActive) {
                    isRecognitionActive = false;
                    recognition.stop();
                    speechStatus.textContent = "Processing...";
                }
                
                // Create and handle WAV blob
                rec.exportWAV(function(blob) {
                    recordedBlob = blob;
                    
                    // Set up audio player
                    const url = URL.createObjectURL(blob);
                    audioPlayer.src = url;
                    
                    // Automatically analyze audio with transcript
                    analyzeRecordingWithTranscript(blob, finalTranscript.trim());
                });
            }

            // Analyze the recording with transcript
            function analyzeRecordingWithTranscript(blob, transcript) {
                loadingIndicator.style.display = 'block';
                resultsContainer.style.display = 'none';
                
                const formData = new FormData();
                formData.append('file', blob, 'recording.wav');
                formData.append('transcript', transcript);
                console.log(transcript);
                
                
                fetch('/upload', {
                    method: 'POST',
                    body: formData
                })
                .then(response => response.json())
                .then(data => {
                    displayResults(data);
                })
                .catch(error => {
                    console.error('Error:', error);
                    alert('Error analyzing audio. Please try again.');
                    loadingIndicator.style.display = 'none';
                });
            }

            // Upload form submit handler
            uploadForm.addEventListener('submit', function(e) {
                e.preventDefault();
                
                if (!audioFileInput.files[0]) {
                    alert('Please select an audio file.');
                    return;
                }
                
                loadingIndicator.style.display = 'block';
                resultsContainer.style.display = 'none';
                
                const formData = new FormData();
                formData.append('file', audioFileInput.files[0]);
                
                fetch('/upload', {
                    method: 'POST',
                    body: formData
                })
                .then(response => response.json())
                .then(data => {
                    displayResults(data);
                })
                .catch(error => {
                    console.error('Error:', error);
                    alert('Error analyzing audio. Please try again.');
                    loadingIndicator.style.display = 'none';
                });
            });

            // Display results function
            function displayResults(data) {
                console.log(data);
                
                loadingIndicator.style.display = "none";
                resultsContainer.style.display = "block";
                resultsContent.innerHTML = "";
                
                if (data.error) {
                    resultsContent.innerHTML = `<div class="alert alert-danger">${data.error}</div>`;
                    return;
                }
                
                resultsContent.innerHTML = `
                    <div class="alert alert-info">
                        <div class="mt-3">
                            <div class="mb-2">image</div>
                            <div class="mb-2">
                                <img src="${data.image}" alt="image" width="512" height="512">
                            </div>
                            <div class="mb-2">Transcript</div>
                            <div class="mb-2">
                                <h2>${data.transcript || "Not available"}</h2>
                            </div>

                            <h5>Emotion Probabilities</h5>
                            <div class="mb-2">Anger</div>
                            <div class="progress mb-2">
                                <div class="progress-bar bg-danger" style="width: ${((data.probabilities?.anger || 0) * 100).toFixed(1)}%;">${((data.probabilities?.anger || 0) * 100).toFixed(1)}%</div>
                            </div>
                               
                            <div class="mb-2">Frustration</div>
                            <div class="progress mb-2">
                                <div class="progress-bar bg-dark" style="width: ${((data.probabilities?.frustration || 0) * 100).toFixed(1)}%;">${((data.probabilities?.frustration || 0) * 100).toFixed(1)}%</div>
                            </div>
                            
                            <div class="mb-2">Happiness</div>
                            <div class="progress mb-2">
                                <div class="progress-bar bg-success" style="width: ${((data.probabilities?.happiness || 0) * 100).toFixed(1)}%;">${((data.probabilities?.happiness || 0) * 100).toFixed(1)}%</div>
                            </div>
                            
                            <div class="mb-2">Neutral</div>
                            <div class="progress mb-2">
                                <div class="progress-bar bg-secondary" style="width: ${((data.probabilities?.neutral || 0) * 100).toFixed(1)}%;">${((data.probabilities?.neutral || 0) * 100).toFixed(1)}%</div>
                            </div>
                            
                            <div class="mb-2">Sadness</div>
                            <div class="progress mb-2">
                                <div class="progress-bar bg-info" style="width: ${((data.probabilities?.sadness || 0) * 100).toFixed(1)}%;">${((data.probabilities?.sadness || 0) * 100).toFixed(1)}%</div>
                            </div>
                        </div>
                    </div>
                `;
            }

            // Event listeners
            recordButton.addEventListener('click', startRecording);
            pauseButton.addEventListener('click', pauseRecording);
            stopButton.addEventListener('click', stopRecording);

            // Initialize visualization
            initAudioVisualization();
        });
    </script>
</body>

</html>